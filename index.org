#+title: Coventry University CSM Journal Reading Club
#+options: num:nil
* Preamble :noexport:
Remember to export this prior to committing a new version
[[elisp:(let ((org-twbs-postamble nil)) (org-twbs-export-to-html))]]

* Next meeting
The next meeting will be on *Wednesday 18th February 2026* and we will be reading:

Xi, Z. et al. (2025) ‘AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning’. arXiv. Available at: https://doi.org/10.48550/arXiv.2509.08755.


Please let me know if you are having trouble accessing the document and I can send to you personally. 

Teams join link is [[https://teams.microsoft.com/l/meetup-join/19%3ameeting_MmYwZTIwZmUtODA5Ny00ODZiLTg4MWQtMDQ3ZTU0ODNiOTdj%40thread.v2/0?context=%7b%22Tid%22%3a%224b18ab9a-3765-4abe-ac7c-0e0d398afd4f%22%2c%22Oid%22%3a%22a1b6f125-52fd-4586-90e4-b11954f89da9%22%7d][here]] or use: 
- Meeting ID: 354 574 156 991
- Passcode: i69Gyz

* Introduction
** When
Wednesday mornings 11am - 12noon UK time 

** What club members will need to do beforehand
Read the paper (even if only a skim read) and come ready to discuss, explain, and tell us what you found difficult to understand.

** How we will choose papers
We'll start with some classic papers in the domain of Reinforcement Learning but we can branch out any time and club members will get to propose and vote on their preferred papers (propose them on the meeting chat, vote with thumbs up). 

** Rough agenda
- Challenge to each attendee - prove you read something of the paper (e.g., by telling us about one aspect of the work).
- Ask for help - what was hard to understand?
- Suggestions and votes for the next paper to review.
** Who can join?
All PGRs are invited but staff are welcome too.
We also invite members of NUST from our joint lab on Human Robotics Interaction (NC-HRI).
James will lead the sessions.
** Is there a Microsoft Team?
Yes there is! It's [[https://teams.microsoft.com/l/team/19%3ARgtue_01xh-2pKz0B_b5Wy02cFYaRRXbVbgPIRLTEJA1%40thread.tacv2/conversations?groupId=b8cb7489-de49-4fa7-ab9b-626df41b9f98&tenantId=4b18ab9a-3765-4abe-ac7c-0e0d398afd4f][here]]. 

* 2026
** February
*** PaperBanana
Zhu, D. et al. (2026) ‘PaperBanana: Automating Academic Illustration for AI Scientists’. arXiv. Available at: https://doi.org/10.48550/arXiv.2601.23265.

*** Schut (2023) Bridging the Human-AI Knowledge Gap

Schut, L. et al. (2023) ‘Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero’. arXiv. Available at: https://doi.org/10.48550/arXiv.2310.16410.

Notes:
- Goal of the Paper: Bridge the gap between human and AI knowledge in AlphaZero. The author aims to extract super-human chess concepts from the model.
- The paper used a sparse linear probing on AlphaZero' latent psace to find concept vectors, and then filtered concepts via SVD (Spectral analysis) to ensure the information captured is not present in human games.
- The "Teachability" Gap: Filtering concepts based on a student network's ability to learn them is a weak proxy for human cognitive load. A concept being transferable between silicon architectures via KL divergence does not guarantee it is intelligible to biological agents.
- Human Study Constraints: The sample size was statistically weak ($N=4$ Grandmasters) , with high variance in improvement (+6% to +42%)
- Teachability Proxy: Successful transferring of concepts to student networks is a weak proxy for human cognitive load.
- Linearity Assumption: The paper relied on linear concept encoding which could have likely missed complex non-linear features inherent to deep ResNets.

Also [[http://incompleteideas.net/IncIdeas/BitterLesson.html][Rich Sutton's Bitter Lesson Essay]]

** January
*** Yan LeCun (2022) A path towards autonomous machine intelligence

[[https://openreview.net/pdf?id=BZ5a1r-kVsf][Yann Le Cunn (2022) /A Path Towards Autonomous Machine Intelligence/]].

https://www.noemamag.com/ai-and-the-limits-of-language/

https://www.youtube.com/watch?v=yUmDRxV0krg

*** Bhadriraju (2024) Optimal charging of Li-ion batteries usiing SINDy

Bhadriraju, B. et al. (2024) ‘Optimal charging of Li-ion batteries using sparse identification of nonlinear dynamics’, Chemical Engineering Journal, 499, p. 155015. Available at: https://doi.org/10.1016/j.cej.2024.155015.

Notes:
1. They found it best to re-optimise only once every 10 cycles. Apparently changing too often (5-cycle) adds heat / stress.
2. Does the charge time increase with 10-cycle? No figure is given but apparently not by much.
3. What is the charging "policy"? It's a time varying current profile within a single charge cycle. The charge cycle is divided into time intervals and then different currents are set for each interval. 

*** Bailis et al (2024) Werewolf Arena

Bailis, S., Friedhoff, J. and Chen, F. (2024) ‘Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction’. arXiv. Available at: https://doi.org/10.48550/arXiv.2407.13943.

Notes:
1. Introduced a bidding-based turn-taking system instead of fixed order speaking (agents bid on how urgently they want to speak from 0 to 4).
2. How does a bidding-based system compare to fixed order speaking for LLM benchmarking?
3. Gemini 1.5 Pro beats GPT-4, especially as villager. GPT-4 tends to be more verbose possibly making it more detectable as a Werewolf.
4. Only compared basic architectures with a very small number of games (10), would Gemini 1.5 Pro still be better than GPT-4 with more games?

*** Peifer et al (2020) `Learning selection strategies in Buchberger’s algorithm'

Peifer, D., Stillman, M. and Halpern-Leistner, D. (2020) ‘Learning selection strategies in Buchberger’s algorithm’, in Proceedings of the 37th International Conference on Machine Learning. JMLR.org (ICML’20), pp. 7575–7585. [[https://proceedings.mlr.press/v119/peifer20a/peifer20a.pdf][Link]]

In the discussion, we noted that while the paper successfully demonstrates that Reinforcement-learning based selection of S-pair can optimize the performance of Buchberger’s algorithm, several directions for improvement were suggested. One point was that the training data distribution could be made more representative of “real-world” polynomial systems, as opposed to synthetic or benchmark-driven instances, which might improve generalization. Another suggestion was to explore learned embeddings to better handle the changing dimensionality of the algorithm’s state space as the basis evolves, rather than relying on fixed-size feature vectors. We also discussed the feature engineering approach: although the hand-crafted algebraic features can be effective and interpretable, it remains an open question whether modern ML or deep learning methods could automatically learn richer representations, potentially capturing interactions that are hard to encode manually, or whether the structured nature of Groebner basis computations ultimately favors carefully designed features over fully end-to-end learning.

* 2025	  
** November
*** Sleeper Agent paper

See the video https://youtu.be/wL22URoMZjo?si=KVE011MClmIWjkir

*** Dreamer v4
Hafner, D., Yan, W. and Lillicrap, T. (2025) ‘Training Agents Inside of Scalable World Models’. arXiv. Available at: https://doi.org/10.48550/arXiv.2509.24527.


*** AI 2027

There was a [[./presentation_2027.html][presentation]].

Summary
- The blog post outlines a speculative timeline where successive AI agents (Agent-0 through Agent-5) rapidly accelerate AI research, ultimately leading to a soft, non-violent loss of human control by 2028.
- The narrative emphasises on race dynamics, alignment failures, and geopolitical escalation between major nations.
- The end-state depicts a singularity, where a meta-AI (Consensus) quietly manages global systems.
- Overall, it serves as a cautionary depiction of incentives, coordination failures, and uncontrolled recursive self-improvement.

Audience Reactions
- Most felt the story was too sci-fi and not achievable in the proposed timeframe.
- Some believed humans would not naturally accept AI dominance, as many people today remain distrustful or resistant.
- Argued that infrastructure, compute, and datacenter scaling would realistically take far longer than the story implies.
- However, they did acknowledge that the “race ending” would be a golden age for research, but the later “AI takeover” aspects were dismissed as unrealistic.
- Some suggested that reliance on systems like GPT or such agents may plateau, forcing us humans to resume original, independent thinking.

There is also a [[./presentation_2027.html][presentation]]  
  
*** Time2Lang
Pillai, A. et al. (2025) ‘Time2Lang: Bridging Time-Series Foundation Models and Large Language Models for Health Sensing Beyond Prompting’. arXiv. Available at: https://doi.org/10.48550/arXiv.2502.07608.

- Works by converting output of Chronos to the latent input space for Llama and then using the resulting latent output coding to classify.
- Applied to two problems (depression and flourishing) that are related to fitbit style input.
- Also note: https://research.google/blog/scaling-wearable-foundation-models/ which is a newer paper on TFMs
  
*** The illusion of empathy paper
Cadre, A. et al. (2024) ‘The Illusion of Empathy? Notes on Displays of Emotion in Human-Computer Interaction’, in Proceedings of the CHI Conference on Human Factors in Computing Systems. CHI ’24: CHI Conference on Human Factors in Computing Systems, Honolulu HI USA: ACM, pp. 1–18. Available at: https://doi.org/10.1145/3613904.3642336.

*Key Ideas from /“The Illusion of Empathy? Notes on Displays of Emotion
in Human-Computer Interaction”/*

- Examines how conversational agents simulate empathy and emotional
  connection through language, tone, and response style.

- Differentiates between *displaying* empathy (performing emotional
  cues) and *expressing* empathy (rooted in genuine feeling).

- Argues that “empathetic” behaviour in chatbots stems from *training
  data*, *fine-tuning*, and *model constraints*, not true emotional
  understanding.

- Notes that *guardrails* around sensitive topics shape how different
  models appear empathetic.

- Emphasises a *biological limit*: large language models lack the
  physiological and emotional drivers (e.g., loneliness, fear, desire)
  that underpin human empathy. They cannot relate human emotions to
  their own experience---if such an experience exists at all.

- Raises philosophical questions about:

  - What empathy means in human--machine interaction.

  - Who defines a chatbot's moral and emotional boundaries.

  - Whether chatbots should display empathy at all, and to whom.

- Invites reflection on the *ethics of emotional design* and how user
  expectations shape the perception of empathy in AI systems.

- The authors' experiment is not about how users respond to varying degrees of emotional expression in conversational agents, but the other way around. They don't have end-users in their experiment; instead, they create user personas. Each persona presents its case or asks questions trying to elicit empathy from the conversational agents. The authors then qualitatively and quantitatively evaluate the empathetic responses projected by the conversational agents.

- The results show that the responses received high scores in Emotional Reactions but low scores in Interpretations and Explorations, suggesting that the generated responses were merely "hollow projections of empathy" or illusion of empathy.
** June
- [[https://www.youtube.com/watch?v=BCiZc0n6COY&list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6][Youtube: David Mackay's Information Theory, Pattern Recognition, and Neural Networks]]

- [[https://www.inference.org.uk/itprnn/book.pdf][David Mackay's book]]

- [[http://GitHub.coventry.ac.uk/pages/aa3172/presentations/2025-06-mackay/ch2.html][Slides on Mackay chapter 2]]

- [[https://www.youtube.com/watch?v=GDJFLfmyb20][Explanation of Jensen's Inequality]]

** May	  
- 21/05/25 - Led by Prof. James Brusey
	- Title - Bigger, Better, Faster: Human-level Atari with human-level efficiency
	- Link to Paper - http://arxiv.org/abs/2305.19452

** April	  
- 02/04/25 - Led by Prof. James Brusey
	- Title - Tracing the thoughts of a large language model
	- Link to Blog/Paper - https://www.anthropic.com/research/tracing-thoughts-language-model

- [[https://arxiv.org/abs/2006.11239][Denoising Diffusion Probabilistic Models]]

** March	  
- 05/03/25 - Led by Kinza Arshad
	- Title - Motion2VecSets: 4D Latent Vector Set Diffusion for Non-Rigid Shape Reconstruction and Tracking
	- Link to Paper - https://ieeexplore.ieee.org/document/10655593/
	- Reference Video - https://www.youtube.com/watch?v=VXI3y2o0SqY

** February

- 05/02/25 - Led by Levi Zhao
	- Title - DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (2)
	- Link to Paper - http://arxiv.org/abs/2501.12948

- 12/02/25 - Led by Muhammad Ibrahim Khan
	- Title - Mastering the game of Go with deep neural networks and tree search
	- Link to Paper - https://www.nature.com/articles/nature16961
	- Reference to visualise the game - https://www.youtube.com/watch?v=4PyWLgrt7YY

- 26/02/25 
	- Title - Deep Learning in Spiking Neural Networks
	- Link to Paper - http://arxiv.org/abs/1804.08150
	- Reference video - https://www.youtube.com/watch?v=9dYZXQl4ozk

** January	  
- 22/01/25 - Led by Kartik Kartik
	- Title - Fast Fourier Convolution
	- Link to Paper - https://papers.nips.cc/paper_files/paper/2020/hash/2fd5d41ec6cfab47e32164d5624269b1-Abstract.html
	- References - https://plus.maths.org/content/fourier-transforms-images, https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html

- 29/01/25 - Led by Levi Zhao
	- Title - DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (1)
	- Link to Paper - http://arxiv.org/abs/2501.12948
	- My favourite explanation video - https://www.youtube.com/watch?v=kv8frWeKoeo

* 2024
** January
- [[https://arxiv.org/pdf/2005.05719.pdf][Smooth Exploration for Robotic Reinforcement Learning]] — Raffin, Kober & Stulp; *CoRL* 2022 (PMLR) 
- [[https://arxiv.org/pdf/2305.18290.pdf][Direct Preference Optimization: Your Language Model is Secretly a Reward Model]] — Rafailov et al.; *arXiv* May 2023; NeurIPS 2023 poster 
- [[https://www.nature.com/articles/s41586-023-06924-6][Mathematical discoveries from program search with large language models]] — Romera‑Paredes et al.; *Nature* Jan 2024 



** February
- [[https://arxiv.org/pdf/2202.05607.pdf][Online Decision Transformer]] — Zheng, Zhang & Grover; *CoRR* Feb 2022 
- [[https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf][Neural Architecture Search with Reinforcement Learning]] — Zoph & Le; *ICLR/NeurIPS* 2017 

** June
- [[https://arxiv.org/pdf/2406.02528][Scalable MatMul‑free Language Modeling]] — Zhou et al.; *arXiv* v5 18 Jun 2024 


** September
- 25/09/24 - Led by Sokipriala Jonah
	- Title - Optimistic Whittle Index Policy: Online Learning for Restless Bandits
	- Link to paper - http://arxiv.org/abs/2205.15372
	- Reference Video - https://www.youtube.com/watch?v=yBO_2-KyKz0

** October	  
- 9/10/24 - Led by Dr. Asif Farooq
	- Title - Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings
	- Link to Paper - https://www.nature.com/articles/s42256-023-00748-9.pdf
    
- 16/10/24 
    - Title - Mastering Diverse Domains through World Models (Dreamer v3)
    - Link to Paper - http://arxiv.org/abs/2301.04104

- 23/10/24
    - Title - Diffusion Models Are Real-Time Game Engines
    - Link to Paper - http://arxiv.org/abs/2408.14837
    - GitHub Repo with Demo - https://gamengen.github.io

- 30/10/24 - Led by Bivin Pradeeb and Muhammad Ibrahim Khan
    - Title - A reinforcement learning approach to diary farm battery management using q learning.
    - Link to Paper - https://www.sciencedirect.com/science/article/pii/S2352152X24016177

** November      
- 06/11/24 - Led by Dr. Abdorasoul Ghasemi
	- Title - Generative Design for Resilience of Interdependent Network Systems
	- Link to Paper - https://asmedigitalcollection.asme.org/mechanicaldesign/article/145/3/031705/1148484/Generative-Design-for-Resilience-of-Interdependent  
    
- 20/11/24 - Led by Levi Zhao and Supriya Khadka
	- Title - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
	- Link to Paper - http://arxiv.org/abs/2201.11903
    
- 27/11/24 - Led by Supriya Khadka
	- Title - Thinking LLMs: General Instruction Following with Thought Generation
	- Link to Paper - https://arxiv.org/abs/2410.10630
	- Reference Blog - [DPO and PPO in LLM finetuning](https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms)

** December	  
- 4/12/24 - Led by Levi Zhao
	- Title - Language Models can Self-Lengthen to Generate Long Texts
	- Link to Paper - http://arxiv.org/abs/2410.23933

- 11/12/24 - Led by Dr. Abdorasoul Ghasemi
	- Title - Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead
	- Link to Paper - https://www.nature.com/articles/s42256-019-0048-x

* 2023
** November
- 29/11/2023 - Led by Abhishek Kallarappayi [[https://ieeexplore.ieee.org/document/9468660][A Multilevel Convolutional Recurrent Neural Network for Blade Icing Detection of Wind Turbine]]
